{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-01T03:26:28.868865Z","iopub.execute_input":"2023-10-01T03:26:28.869204Z","iopub.status.idle":"2023-10-01T03:26:29.320791Z","shell.execute_reply.started":"2023-10-01T03:26:28.869174Z","shell.execute_reply":"2023-10-01T03:26:29.319349Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"training_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:26:29.322810Z","iopub.execute_input":"2023-10-01T03:26:29.323202Z","iopub.status.idle":"2023-10-01T03:26:29.370105Z","shell.execute_reply.started":"2023-10-01T03:26:29.323176Z","shell.execute_reply":"2023-10-01T03:26:29.369268Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"training_df.info()\ntest_df.info()\ntraining_df.describe()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:26:29.371286Z","iopub.execute_input":"2023-10-01T03:26:29.372072Z","iopub.status.idle":"2023-10-01T03:26:29.413471Z","shell.execute_reply.started":"2023-10-01T03:26:29.372034Z","shell.execute_reply":"2023-10-01T03:26:29.412213Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7613 entries, 0 to 7612\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        7613 non-null   int64 \n 1   keyword   7552 non-null   object\n 2   location  5080 non-null   object\n 3   text      7613 non-null   object\n 4   target    7613 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 297.5+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 102.1+ KB\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                 id      target\ncount   7613.000000  7613.00000\nmean    5441.934848     0.42966\nstd     3137.116090     0.49506\nmin        1.000000     0.00000\n25%     2734.000000     0.00000\n50%     5408.000000     0.00000\n75%     8146.000000     1.00000\nmax    10873.000000     1.00000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7613.000000</td>\n      <td>7613.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5441.934848</td>\n      <td>0.42966</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3137.116090</td>\n      <td>0.49506</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2734.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5408.000000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8146.000000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>10873.000000</td>\n      <td>1.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Question 1a\n\nThere are 7613 training data points, and 3263 test data points.\n42.966% of the training tweets are real disasters, and 57.034% are not.","metadata":{}},{"cell_type":"markdown","source":"Question 1c\n\nPreprocessing the full training data before splitting, because eventually we would need to preprocess all of the training and test data anyway.\nThe preprocessing measures taken are:\n1) converting all words to lowercase -> to avoid duplicate features of the same word due to different casing\n\n2) removing the URLs - many url's are not comprising meaningful English words from which we can deduce the context of the tweet, and so removing the URLs should help the prediction.\n\n   The URLs are removed before punctuation removal because the '/' are needed to identify them\n   \n3) removing the usernames - same reason as removing URLs. This is also done before stripping punctuation because the '@' symbol is used for the identifications\n\n4) strip the punctuation - for easier lemmatizing later\n\n5) removing the numbers - because in our models used we cannot place the numbers well into context, so removing them as unusable data (for the purpose of the predictions).\n\n6) removing stopwords - stopwords would not add valuable information which would help the predictions.\n\n7) lemmatize all words - reduce the number of features so the model can use them better.","metadata":{}},{"cell_type":"code","source":"import re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n \nstop_words = set(stopwords.words('english'))\n\ndef lowercase(text):\n    #lowercase\n    return text.lower()\n\ndef remove_URL(sample):\n    \"\"\"Remove URLs from a sample string\"\"\"\n    return re.sub(r\"http\\S+\", \"\", sample)\n\ndef remove_usernames(tweet):\n    return re.sub('@[^\\s]+','',tweet)\n\ndef remove_punctuation_translate(text):\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ndef remove_numbers(text):\n    translator = str.maketrans('', '', '0123456789')\n    return text.translate(translator)\n\ndef remove_stopwords(text):\n    str=''\n    word_tokens = word_tokenize(text)\n \n    for w in word_tokens:\n        if w not in stop_words:\n            str = str + w + ' '\n \n    return str\n\ndef clean_text(text):\n    output_str = lowercase(text)\n    output_str = remove_URL(output_str)\n    output_str = remove_usernames(output_str)\n    output_str = remove_punctuation_translate(output_str)\n    output_str = remove_numbers(output_str)\n    output_str = remove_stopwords(output_str)\n    return output_str\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:26:29.416486Z","iopub.execute_input":"2023-10-01T03:26:29.417817Z","iopub.status.idle":"2023-10-01T03:26:30.332394Z","shell.execute_reply.started":"2023-10-01T03:26:29.417778Z","shell.execute_reply":"2023-10-01T03:26:30.331050Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"training_df['text'] = training_df['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:26:30.334100Z","iopub.execute_input":"2023-10-01T03:26:30.334508Z","iopub.status.idle":"2023-10-01T03:26:31.360557Z","shell.execute_reply.started":"2023-10-01T03:26:30.334468Z","shell.execute_reply":"2023-10-01T03:26:31.359046Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"training_df['text'].tail(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:26:31.362113Z","iopub.execute_input":"2023-10-01T03:26:31.362790Z","iopub.status.idle":"2023-10-01T03:26:31.372500Z","shell.execute_reply.started":"2023-10-01T03:26:31.362756Z","shell.execute_reply":"2023-10-01T03:26:31.371082Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"7603                                  officials say quarantine place alabama home possible ebola case developing symptoms \n7604                                     worldnews fallen powerlines glink tram update fire crews evacuated passengers tr \n7605                                                          flip side im walmart bomb everyone evacuate stay tuned blow \n7606                                   suicide bomber kills saudi security site mosque reuters via world google news wall \n7607                                           stormchase violent record breaking ef el reno oklahoma tornado nearly runs \n7608                                                                two giant cranes holding bridge collapse nearby homes \n7609                                                     control wild fires california even northern part state troubling \n7610                                                                                                 utckm volcano hawaii \n7611    police investigating ebike collided car little portugal ebike rider suffered serious nonlife threatening injuries \n7612                                                             latest homes razed northern california wildfire abc news \nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef lemmatizer(text):        \n    sent = []\n    doc = nlp(text)\n    for word in doc:\n        sent.append(word.lemma_)\n    return \" \".join(sent)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:26:31.374603Z","iopub.execute_input":"2023-10-01T03:26:31.375311Z","iopub.status.idle":"2023-10-01T03:26:46.972649Z","shell.execute_reply.started":"2023-10-01T03:26:31.375277Z","shell.execute_reply":"2023-10-01T03:26:46.971325Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"training_df['text'] = training_df['text'].apply(lemmatizer)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:26:46.974148Z","iopub.execute_input":"2023-10-01T03:26:46.974870Z","iopub.status.idle":"2023-10-01T03:27:44.551596Z","shell.execute_reply.started":"2023-10-01T03:26:46.974838Z","shell.execute_reply":"2023-10-01T03:27:44.550251Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"training_df['text'].tail(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.553222Z","iopub.execute_input":"2023-10-01T03:27:44.554211Z","iopub.status.idle":"2023-10-01T03:27:44.561906Z","shell.execute_reply.started":"2023-10-01T03:27:44.554177Z","shell.execute_reply":"2023-10-01T03:27:44.561135Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"7603                             official say quarantine place alabama home possible ebola case develop symptom\n7604                                  worldnew fall powerline glink tram update fire crew evacuate passenger tr\n7605                                                flip side I m walmart bomb everyone evacuate stay tune blow\n7606                           suicide bomber kill saudi security site mosque reuter via world google news wall\n7607                                     stormchase violent record break ef el reno oklahoma tornado nearly run\n7608                                                           two giant crane hold bridge collapse nearby home\n7609                                            control wild fire california even northern part state troubling\n7610                                                                                       utckm volcano hawaii\n7611    police investigate ebike collide car little portugal ebike rider suffer serious nonlife threaten injury\n7612                                                       late home raze northern california wildfire abc news\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"training_target = training_df['target']\ntraining_df.drop('target', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.565706Z","iopub.execute_input":"2023-10-01T03:27:44.566898Z","iopub.status.idle":"2023-10-01T03:27:44.582173Z","shell.execute_reply.started":"2023-10-01T03:27:44.566821Z","shell.execute_reply":"2023-10-01T03:27:44.581049Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Question 1b:\nThe next cell splits the training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_dev, y_train, y_dev = train_test_split(training_df,training_target,random_state=104,test_size=0.3,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.583982Z","iopub.execute_input":"2023-10-01T03:27:44.584434Z","iopub.status.idle":"2023-10-01T03:27:44.600752Z","shell.execute_reply.started":"2023-10-01T03:27:44.584386Z","shell.execute_reply":"2023-10-01T03:27:44.599499Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"x_train.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.602702Z","iopub.execute_input":"2023-10-01T03:27:44.603042Z","iopub.status.idle":"2023-10-01T03:27:44.623992Z","shell.execute_reply.started":"2023-10-01T03:27:44.603014Z","shell.execute_reply":"2023-10-01T03:27:44.622334Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 5329 entries, 6922 to 69\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        5329 non-null   int64 \n 1   keyword   5285 non-null   object\n 2   location  3568 non-null   object\n 3   text      5329 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 208.2+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"x_dev.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.625698Z","iopub.execute_input":"2023-10-01T03:27:44.627069Z","iopub.status.idle":"2023-10-01T03:27:44.640788Z","shell.execute_reply.started":"2023-10-01T03:27:44.627026Z","shell.execute_reply":"2023-10-01T03:27:44.639448Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 2284 entries, 7071 to 3689\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        2284 non-null   int64 \n 1   keyword   2267 non-null   object\n 2   location  1512 non-null   object\n 3   text      2284 non-null   object\ndtypes: int64(1), object(3)\nmemory usage: 89.2+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Question 1d from here","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(min_df = 10, binary=True)\nx_train_bow = vectorizer.fit_transform(x_train['text'])\nprint(x_train_bow.toarray().shape)\n\nfeatures = list(vectorizer.get_feature_names_out())\nprint(len(features))\nprint(features[:15])\n\ntransformed_x_train = vectorizer.transform(x_train['text'])\nprint(transformed_x_train.shape)\n\npd.DataFrame(transformed_x_train.toarray()).head(5)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.642514Z","iopub.execute_input":"2023-10-01T03:27:44.642931Z","iopub.status.idle":"2023-10-01T03:27:44.883781Z","shell.execute_reply.started":"2023-10-01T03:27:44.642896Z","shell.execute_reply":"2023-10-01T03:27:44.882211Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(5329, 1004)\n1004\n['abandon', 'abc', 'ablaze', 'absolutely', 'accident', 'across', 'act', 'action', 'actually', 'add', 'affect', 'aftershock', 'ago', 'agree', 'air']\n(5329, 1004)\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   0     1     2     3     4     5     6     7     8     9     ...  994   \\\n0     0     0     0     0     0     0     0     0     0     0  ...     0   \n1     0     0     0     0     0     0     0     0     0     0  ...     0   \n2     0     0     0     0     0     0     0     0     0     0  ...     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0   \n\n   995   996   997   998   999   1000  1001  1002  1003  \n0     0     0     0     0     0     0     0     0     0  \n1     0     0     0     0     0     0     0     0     0  \n2     0     0     0     0     0     0     0     0     0  \n3     0     0     0     0     0     0     0     0     0  \n4     0     0     0     0     0     0     0     0     0  \n\n[5 rows x 1004 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>994</th>\n      <th>995</th>\n      <th>996</th>\n      <th>997</th>\n      <th>998</th>\n      <th>999</th>\n      <th>1000</th>\n      <th>1001</th>\n      <th>1002</th>\n      <th>1003</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1004 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"transformed_x_dev = vectorizer.transform(x_dev['text'])\nprint(transformed_x_dev.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.885575Z","iopub.execute_input":"2023-10-01T03:27:44.886044Z","iopub.status.idle":"2023-10-01T03:27:44.921384Z","shell.execute_reply.started":"2023-10-01T03:27:44.886001Z","shell.execute_reply":"2023-10-01T03:27:44.920009Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(2284, 1004)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import BernoulliNB","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.923138Z","iopub.execute_input":"2023-10-01T03:27:44.923464Z","iopub.status.idle":"2023-10-01T03:27:44.939022Z","shell.execute_reply.started":"2023-10-01T03:27:44.923438Z","shell.execute_reply":"2023-10-01T03:27:44.937496Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Question 1e part i here","metadata":{}},{"cell_type":"code","source":"model1 = LogisticRegression(penalty = 'none', max_iter=1500)\nmodel1.fit(transformed_x_train, y_train)\nf1_score1_train = f1_score(y_train, model1.predict(transformed_x_train))\nprint(f1_score1_train)\n\nf1_score1_dev = f1_score(y_dev, model1.predict(transformed_x_dev))\nprint(f1_score1_dev)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:44.941068Z","iopub.execute_input":"2023-10-01T03:27:44.941439Z","iopub.status.idle":"2023-10-01T03:27:45.441328Z","shell.execute_reply.started":"2023-10-01T03:27:44.941412Z","shell.execute_reply":"2023-10-01T03:27:45.439768Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"0.8446755595749491\n0.704652378463147\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Question 1e part ii here","metadata":{}},{"cell_type":"code","source":"model_L1_train = LogisticRegression(penalty='l1', solver='liblinear')\nmodel_L1_train.fit(transformed_x_train, y_train)\nf1_score_L1_train = f1_score(y_train, model_L1_train.predict(transformed_x_train))\nprint(f1_score_L1_train)\nf1_score1_L1_dev = f1_score(y_dev, model_L1_train.predict(transformed_x_dev))\nprint(f1_score1_L1_dev)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.442954Z","iopub.execute_input":"2023-10-01T03:27:45.443323Z","iopub.status.idle":"2023-10-01T03:27:45.470816Z","shell.execute_reply.started":"2023-10-01T03:27:45.443295Z","shell.execute_reply":"2023-10-01T03:27:45.469031Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"0.8121637426900585\n0.7236624379481522\n","output_type":"stream"}]},{"cell_type":"code","source":"model_L2_train = LogisticRegression(penalty='l2', solver='lbfgs')\n\nmodel_L2_train.fit(transformed_x_train, y_train)\n\nf1_score_L2_train = f1_score(y_train, model_L2_train.predict(transformed_x_train))\nprint(f1_score_L2_train)\nf1_score1_L2_dev = f1_score(y_dev, model_L2_train.predict(transformed_x_dev))\nprint(f1_score1_L2_dev)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.472757Z","iopub.execute_input":"2023-10-01T03:27:45.473828Z","iopub.status.idle":"2023-10-01T03:27:45.550207Z","shell.execute_reply.started":"2023-10-01T03:27:45.473796Z","shell.execute_reply":"2023-10-01T03:27:45.548760Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"0.8198930978387171\n0.7268746579091406\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model_L1_train.coef_[0])\n\nmodel_L1_coef_df = pd.DataFrame()\nmodel_L1_coef_df['words'] = features\nmodel_L1_coef_df['weight'] = model_L1_train.coef_[0]\nprint(model_L1_coef_df.sort_values(by='weight', ascending = False).head(10))\nprint(model_L1_coef_df.sort_values(by='weight').head(10))","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.551965Z","iopub.execute_input":"2023-10-01T03:27:45.552303Z","iopub.status.idle":"2023-10-01T03:27:45.569086Z","shell.execute_reply.started":"2023-10-01T03:27:45.552275Z","shell.execute_reply":"2023-10-01T03:27:45.567569Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[0.         0.         0.         ... 0.         0.36711927 0.        ]\n          words    weight\n623    outbreak  4.616913\n799       spill  4.335677\n907     typhoon  4.127481\n963    wildfire  4.047786\n110     bombing  3.969742\n545     migrant  3.363149\n259  earthquake  3.328956\n263       ebola  3.184166\n407   hiroshima  3.078705\n569      murder  3.046417\n        words    weight\n982     write -2.427214\n127       buy -2.050464\n554      mode -1.785238\n618    online -1.764904\n175  complete -1.618294\n758      self -1.609962\n790      song -1.519384\n129      cake -1.416140\n507      long -1.410451\n863     throw -1.398161\n","output_type":"stream"}]},{"cell_type":"code","source":"n = transformed_x_train.shape[0] # size of the dataset\nd = transformed_x_train.shape[1] # number of features in our dataset\nprint(n)\nprint(d)\n\nK = 2 # number of clases\n\n# these are the shapes of the parameters\npsis = np.zeros([K,d])\nphis = np.zeros([K])\n\n# we now compute the parameters\nfor k in range(K):\n    X_k = transformed_x_train[y_train == k]\n    psis[k] = np.mean(X_k, axis=0)\n    phis[k] = X_k.shape[0] / float(n)\n\n# print out the class proportions\nprint(phis)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.570776Z","iopub.execute_input":"2023-10-01T03:27:45.571144Z","iopub.status.idle":"2023-10-01T03:27:45.585945Z","shell.execute_reply.started":"2023-10-01T03:27:45.571115Z","shell.execute_reply":"2023-10-01T03:27:45.584477Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"5329\n1004\n[0.56802402 0.43197598]\n","output_type":"stream"}]},{"cell_type":"code","source":"def nb_predictions(x, psis, phis):\n    \"\"\"This returns class assignments and scores under the NB model.\n    \n    We compute \\arg\\max_y p(y|x) as \\arg\\max_y p(x|y)p(y)\n    \"\"\"\n    # adjust shapes\n    n, d = x.shape\n    print(n)\n    print(d)\n    x = np.reshape(x.toarray(), (1, n, d))\n    psis = np.reshape(psis, (K, 1, d))\n    \n    # clip probabilities to avoid log(0)\n    psis = psis.clip(1e-14, 1-1e-14)\n    \n    # compute log-probabilities\n    logpy = np.log(phis).reshape([K,1])\n    logpxy = x * np.log(psis) + (1-x) * np.log(1-psis)\n    logpyx = logpxy.sum(axis=2) + logpy\n\n    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K,n])\n\nidx, logpyx = nb_predictions(transformed_x_dev, psis, phis)\nprint(idx[:10])","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.587475Z","iopub.execute_input":"2023-10-01T03:27:45.587834Z","iopub.status.idle":"2023-10-01T03:27:45.699278Z","shell.execute_reply.started":"2023-10-01T03:27:45.587806Z","shell.execute_reply":"2023-10-01T03:27:45.697661Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"2284\n1004\n[0 1 0 1 1 0 0 1 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"(idx==y_dev).mean()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.700767Z","iopub.execute_input":"2023-10-01T03:27:45.701106Z","iopub.status.idle":"2023-10-01T03:27:45.709858Z","shell.execute_reply.started":"2023-10-01T03:27:45.701079Z","shell.execute_reply":"2023-10-01T03:27:45.708664Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"0.7850262697022767"},"metadata":{}}]},{"cell_type":"code","source":"f1_score_BNB = f1_score(y_dev, idx)\nprint(f1_score_BNB)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.711942Z","iopub.execute_input":"2023-10-01T03:27:45.712304Z","iopub.status.idle":"2023-10-01T03:27:45.727415Z","shell.execute_reply.started":"2023-10-01T03:27:45.712277Z","shell.execute_reply":"2023-10-01T03:27:45.726267Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"0.7291781577495863\n","output_type":"stream"}]},{"cell_type":"code","source":"vectorizer_ngram = CountVectorizer(ngram_range=(2,2), min_df=3)\nx_train_ngram = vectorizer_ngram.fit_transform(x_train['text'])\nprint(x_train_ngram.toarray().shape)\n\nfeatures_ngram = list(vectorizer_ngram.get_feature_names_out())\nprint(len(features_ngram))\nprint(features_ngram[:15])\n\ntransformed_x_train_ngram = vectorizer_ngram.transform(x_train['text'])\nprint(transformed_x_train_ngram.shape)\n\npd.DataFrame(transformed_x_train_ngram.toarray()).head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.729114Z","iopub.execute_input":"2023-10-01T03:27:45.730411Z","iopub.status.idle":"2023-10-01T03:27:45.980122Z","shell.execute_reply.started":"2023-10-01T03:27:45.730368Z","shell.execute_reply":"2023-10-01T03:27:45.978944Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"(5329, 1592)\n1592\n['aba woman', 'abandon aircraft', 'abbswinston zionist', 'abc news', 'access secret', 'accident expert', 'accident indian', 'accident man', 'accident property', 'accuse nema', 'act mass', 'action hostage', 'action year', 'activate municipal', 'add video']\n(5329, 1592)\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"   0     1     2     3     4     5     6     7     8     9     ...  1582  \\\n0     0     0     0     0     0     0     0     0     0     0  ...     0   \n1     0     0     0     0     0     0     0     0     0     0  ...     0   \n2     0     0     0     0     0     0     0     0     0     0  ...     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0   \n5     0     0     0     0     0     0     0     0     0     0  ...     0   \n6     0     0     0     0     0     0     0     0     0     0  ...     0   \n7     0     0     0     0     0     0     0     0     0     0  ...     0   \n8     0     0     0     0     0     0     0     0     0     0  ...     0   \n9     0     0     0     0     0     0     0     0     0     0  ...     0   \n\n   1583  1584  1585  1586  1587  1588  1589  1590  1591  \n0     0     0     0     0     0     0     0     0     0  \n1     0     0     0     0     0     0     0     0     0  \n2     0     0     0     0     0     0     0     0     0  \n3     0     0     0     0     0     0     0     0     0  \n4     0     0     0     0     0     0     0     0     0  \n5     0     0     0     0     0     0     0     0     0  \n6     0     0     0     0     0     0     0     0     0  \n7     0     0     0     0     0     0     0     0     0  \n8     0     0     0     0     0     0     0     0     0  \n9     0     0     0     0     0     0     0     0     0  \n\n[10 rows x 1592 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1582</th>\n      <th>1583</th>\n      <th>1584</th>\n      <th>1585</th>\n      <th>1586</th>\n      <th>1587</th>\n      <th>1588</th>\n      <th>1589</th>\n      <th>1590</th>\n      <th>1591</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 1592 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"transformed_x_dev_ngram = vectorizer_ngram.transform(x_dev['text'])\nprint(transformed_x_dev_ngram.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:45.981893Z","iopub.execute_input":"2023-10-01T03:27:45.982376Z","iopub.status.idle":"2023-10-01T03:27:46.034190Z","shell.execute_reply.started":"2023-10-01T03:27:45.982333Z","shell.execute_reply":"2023-10-01T03:27:46.032794Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(2284, 1592)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Logistic Regression Classifier\nmodel1_ngram = LogisticRegression(penalty = 'none', max_iter=10000)\nmodel1_ngram.fit(transformed_x_train_ngram, y_train)\nf1_score1_train_ngram = f1_score(y_train, model1_ngram.predict(transformed_x_train_ngram))\nprint(f1_score1_train_ngram)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.035707Z","iopub.execute_input":"2023-10-01T03:27:46.036066Z","iopub.status.idle":"2023-10-01T03:27:46.374810Z","shell.execute_reply.started":"2023-10-01T03:27:46.036036Z","shell.execute_reply":"2023-10-01T03:27:46.373395Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"0.6767040788393102\n","output_type":"stream"}]},{"cell_type":"code","source":"model1_ngram_L1 = LogisticRegression(penalty='l1', solver='liblinear')\n\nmodel1_ngram_L1.fit(transformed_x_train_ngram, y_train)\nf1_score1_train_ngram_L1 = f1_score(y_train, model1_ngram_L1.predict(transformed_x_train_ngram))\nprint(f1_score1_train_ngram_L1)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.381266Z","iopub.execute_input":"2023-10-01T03:27:46.381632Z","iopub.status.idle":"2023-10-01T03:27:46.402083Z","shell.execute_reply.started":"2023-10-01T03:27:46.381605Z","shell.execute_reply":"2023-10-01T03:27:46.400598Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"0.6285714285714287\n","output_type":"stream"}]},{"cell_type":"code","source":"model1_ngram_L2 = LogisticRegression(penalty='l2', solver='lbfgs')\n\nmodel1_ngram_L2.fit(transformed_x_train_ngram, y_train)\nf1_score1_train_ngram_L2 = f1_score(y_train, model1_ngram_L2.predict(transformed_x_train_ngram))\nprint(f1_score1_train_ngram_L2)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.403559Z","iopub.execute_input":"2023-10-01T03:27:46.403972Z","iopub.status.idle":"2023-10-01T03:27:46.454371Z","shell.execute_reply.started":"2023-10-01T03:27:46.403934Z","shell.execute_reply":"2023-10-01T03:27:46.452875Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"0.6525303929884083\n","output_type":"stream"}]},{"cell_type":"code","source":"f1_score1_dev_ngram = f1_score(y_dev, model1_ngram.predict(transformed_x_dev_ngram))\nprint(f1_score1_dev_ngram)\n\nf1_score1_dev_ngram_L1 = f1_score(y_dev, model1_ngram_L1.predict(transformed_x_dev_ngram))\nprint(f1_score1_dev_ngram_L1)\n\nf1_score1_dev_ngram_L2 = f1_score(y_dev, model1_ngram_L2.predict(transformed_x_dev_ngram))\nprint(f1_score1_dev_ngram_L2)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.455819Z","iopub.execute_input":"2023-10-01T03:27:46.456150Z","iopub.status.idle":"2023-10-01T03:27:46.476229Z","shell.execute_reply.started":"2023-10-01T03:27:46.456123Z","shell.execute_reply":"2023-10-01T03:27:46.473988Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"0.5667796610169492\n0.5440229062276306\n0.5532212885154062\n","output_type":"stream"}]},{"cell_type":"code","source":"model_ngram_L1_coef_df = pd.DataFrame()\nmodel_ngram_L1_coef_df['words'] = features_ngram\nmodel_ngram_L1_coef_df['weight'] = model1_ngram_L1.coef_[0]\nmodel_ngram_L1_coef_df.sort_values(by='weight', ascending = False).head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.477655Z","iopub.execute_input":"2023-10-01T03:27:46.478681Z","iopub.status.idle":"2023-10-01T03:27:46.492579Z","shell.execute_reply.started":"2023-10-01T03:27:46.478649Z","shell.execute_reply":"2023-10-01T03:27:46.491392Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"                    words    weight\n1337       suicide bomber  3.968303\n1023            oil spill  3.649493\n1445     typhoon soudelor  3.492770\n1338      suicide bombing  3.487778\n1247  severe thunderstorm  3.381132\n82            atomic bomb  3.353852\n1411     train derailment  3.298614\n1410         train derail  3.269008\n611      helicopter crash  3.167348\n268            confirm mh  3.114723","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>words</th>\n      <th>weight</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1337</th>\n      <td>suicide bomber</td>\n      <td>3.968303</td>\n    </tr>\n    <tr>\n      <th>1023</th>\n      <td>oil spill</td>\n      <td>3.649493</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>typhoon soudelor</td>\n      <td>3.492770</td>\n    </tr>\n    <tr>\n      <th>1338</th>\n      <td>suicide bombing</td>\n      <td>3.487778</td>\n    </tr>\n    <tr>\n      <th>1247</th>\n      <td>severe thunderstorm</td>\n      <td>3.381132</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>atomic bomb</td>\n      <td>3.353852</td>\n    </tr>\n    <tr>\n      <th>1411</th>\n      <td>train derailment</td>\n      <td>3.298614</td>\n    </tr>\n    <tr>\n      <th>1410</th>\n      <td>train derail</td>\n      <td>3.269008</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>helicopter crash</td>\n      <td>3.167348</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>confirm mh</td>\n      <td>3.114723</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Bernoulli Classifier\nn_ngram = transformed_x_train_ngram.shape[0] # size of the dataset\nd_ngram = transformed_x_train_ngram.shape[1] # number of features in our dataset\nprint(n_ngram)\nprint(d_ngram)\n\nK_ngram = 2 # number of clases\n\n# these are the shapes of the parameters\npsis_ngram = np.zeros([K_ngram,d_ngram])\nphis_ngram = np.zeros([K_ngram])\n\n# we now compute the parameters\nfor k in range(K_ngram):\n    X_k_ngram = transformed_x_train_ngram[y_train == k]\n    psis_ngram[k] = np.mean(X_k_ngram, axis=0)\n    phis_ngram[k] = X_k_ngram.shape[0] / float(n_ngram)\n\n# print out the class proportions\nprint(phis_ngram)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.494625Z","iopub.execute_input":"2023-10-01T03:27:46.495250Z","iopub.status.idle":"2023-10-01T03:27:46.505648Z","shell.execute_reply.started":"2023-10-01T03:27:46.495221Z","shell.execute_reply":"2023-10-01T03:27:46.504576Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"5329\n1592\n[0.56802402 0.43197598]\n","output_type":"stream"}]},{"cell_type":"code","source":"idx_ngram, logpyx_ngram = nb_predictions(transformed_x_dev_ngram, psis_ngram, phis_ngram)\nprint(idx_ngram[:10])","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.507076Z","iopub.execute_input":"2023-10-01T03:27:46.507518Z","iopub.status.idle":"2023-10-01T03:27:46.626675Z","shell.execute_reply.started":"2023-10-01T03:27:46.507490Z","shell.execute_reply":"2023-10-01T03:27:46.625473Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"2284\n1592\n[0 0 0 0 0 0 0 1 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"(idx_ngram==y_dev).mean()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.628130Z","iopub.execute_input":"2023-10-01T03:27:46.628833Z","iopub.status.idle":"2023-10-01T03:27:46.636043Z","shell.execute_reply.started":"2023-10-01T03:27:46.628805Z","shell.execute_reply":"2023-10-01T03:27:46.634448Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0.7140980735551664"},"metadata":{}}]},{"cell_type":"code","source":"f1_score_BNB_ngram = f1_score(y_dev, idx_ngram)\nprint(f1_score_BNB_ngram)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.637633Z","iopub.execute_input":"2023-10-01T03:27:46.638093Z","iopub.status.idle":"2023-10-01T03:27:46.655078Z","shell.execute_reply.started":"2023-10-01T03:27:46.638064Z","shell.execute_reply":"2023-10-01T03:27:46.654159Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"0.520909757887014\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df['text'] = test_df['text'].apply(clean_text)\ntest_df['text'] = test_df['text'].apply(lemmatizer)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:27:46.656296Z","iopub.execute_input":"2023-10-01T03:27:46.657248Z","iopub.status.idle":"2023-10-01T03:28:10.746351Z","shell.execute_reply.started":"2023-10-01T03:27:46.657218Z","shell.execute_reply":"2023-10-01T03:28:10.745242Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"test_df['text'].head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:28:10.747791Z","iopub.execute_input":"2023-10-01T03:28:10.748118Z","iopub.status.idle":"2023-10-01T03:28:10.757000Z","shell.execute_reply.started":"2023-10-01T03:28:10.748092Z","shell.execute_reply":"2023-10-01T03:28:10.755622Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0                                  happen terrible car crash\n1          hear earthquake different city stay safe everyone\n2        forest fire spot pond geese flee across street save\n3                          apocalypse light spokane wildfire\n4                        typhoon soudelor kills china taiwan\n5                                       shakingit earthquake\n6    they d probably still show life arsenal yesterday eh eh\n7                                                        hey\n8                                                   nice hat\n9                                                       fuck\nName: text, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"vectorizer_fulltraining_ngram = CountVectorizer(ngram_range=(2,2), min_df=3)\ntraining_df_ngram = vectorizer_fulltraining_ngram.fit_transform(training_df['text'])\nprint(training_df_ngram.toarray().shape)\n\nfeatures_fulltraining_ngram = list(vectorizer_fulltraining_ngram.get_feature_names_out())\nprint(len(features_fulltraining_ngram))\nprint(features_fulltraining_ngram[:15])\n\ntransformed_training_df_ngram = vectorizer_fulltraining_ngram.transform(training_df['text'])\nprint(transformed_training_df_ngram.shape)\n\npd.DataFrame(transformed_training_df_ngram.toarray()).head(10)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:28:10.758608Z","iopub.execute_input":"2023-10-01T03:28:10.759012Z","iopub.status.idle":"2023-10-01T03:28:11.073637Z","shell.execute_reply.started":"2023-10-01T03:28:10.758976Z","shell.execute_reply":"2023-10-01T03:28:11.072414Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"(7613, 2422)\n2422\n['aba woman', 'abandon aircraft', 'abbswinston zionist', 'abc news', 'abcnews obama', 'access secret', 'accident expert', 'accident indian', 'accident man', 'accident property', 'account hiroshima', 'accuse nema', 'act mass', 'action hostage', 'action year']\n(7613, 2422)\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"   0     1     2     3     4     5     6     7     8     9     ...  2412  \\\n0     0     0     0     0     0     0     0     0     0     0  ...     0   \n1     0     0     0     0     0     0     0     0     0     0  ...     0   \n2     0     0     0     0     0     0     0     0     0     0  ...     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0   \n5     0     0     0     0     0     0     0     0     0     0  ...     0   \n6     0     0     0     0     0     0     0     0     0     0  ...     0   \n7     0     0     0     0     0     0     0     0     0     0  ...     0   \n8     0     0     0     0     0     0     0     0     0     0  ...     0   \n9     0     0     0     0     0     0     0     0     0     0  ...     0   \n\n   2413  2414  2415  2416  2417  2418  2419  2420  2421  \n0     0     0     0     0     0     0     0     0     0  \n1     0     0     0     0     0     0     0     0     0  \n2     0     0     0     0     0     0     0     0     0  \n3     0     0     0     0     0     0     0     0     0  \n4     0     0     0     0     0     0     0     0     0  \n5     0     0     0     0     0     0     0     0     0  \n6     0     0     0     0     0     0     0     0     0  \n7     0     0     0     0     0     0     0     0     0  \n8     0     0     0     0     0     0     0     0     0  \n9     0     0     0     0     0     0     0     0     0  \n\n[10 rows x 2422 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>2412</th>\n      <th>2413</th>\n      <th>2414</th>\n      <th>2415</th>\n      <th>2416</th>\n      <th>2417</th>\n      <th>2418</th>\n      <th>2419</th>\n      <th>2420</th>\n      <th>2421</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 2422 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Bernoulli Classifier\nn_ft_ngram = transformed_training_df_ngram.shape[0] # size of the dataset\nd_ft_ngram = transformed_training_df_ngram.shape[1] # number of features in our dataset\nprint(n_ft_ngram)\nprint(d_ft_ngram)\n\nK_ft_ngram = 2 # number of clases\n\n# these are the shapes of the parameters\npsis_ft_ngram = np.zeros([K_ft_ngram,d_ft_ngram])\nphis_ft_ngram = np.zeros([K_ft_ngram])\n\n# we now compute the parameters\nfor k in range(K_ft_ngram):\n    X_k_ft_ngram = transformed_training_df_ngram[training_target == k]\n    psis_ft_ngram[k] = np.mean(X_k_ft_ngram, axis=0)\n    phis_ft_ngram[k] = X_k_ft_ngram.shape[0] / float(n_ft_ngram)\n\n# print out the class proportions\nprint(phis_ft_ngram)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:28:11.075290Z","iopub.execute_input":"2023-10-01T03:28:11.075722Z","iopub.status.idle":"2023-10-01T03:28:11.089171Z","shell.execute_reply.started":"2023-10-01T03:28:11.075684Z","shell.execute_reply":"2023-10-01T03:28:11.087592Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"7613\n2422\n[0.57034021 0.42965979]\n","output_type":"stream"}]},{"cell_type":"code","source":"transformed_test_ngram = vectorizer_fulltraining_ngram.transform(test_df['text'])\nprint(transformed_test_ngram.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:28:11.091289Z","iopub.execute_input":"2023-10-01T03:28:11.091801Z","iopub.status.idle":"2023-10-01T03:28:11.142812Z","shell.execute_reply.started":"2023-10-01T03:28:11.091759Z","shell.execute_reply":"2023-10-01T03:28:11.141325Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"(3263, 2422)\n","output_type":"stream"}]},{"cell_type":"code","source":"idx_ft_ngram, logpyx_ft_ngram = nb_predictions(transformed_test_ngram, psis_ft_ngram, phis_ft_ngram)\nprint(idx_ft_ngram[:10])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:28:11.144391Z","iopub.execute_input":"2023-10-01T03:28:11.144738Z","iopub.status.idle":"2023-10-01T03:28:11.480235Z","shell.execute_reply.started":"2023-10-01T03:28:11.144710Z","shell.execute_reply":"2023-10-01T03:28:11.479289Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"3263\n2422\n[0 0 0 0 1 0 0 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"y_test_pred_df = pd.DataFrame(idx_ft_ngram, columns = ['target'])\ntest_ids = test_df['id']\nsubmission_list = pd.concat([test_ids, y_test_pred_df], axis=1, join='inner')\nsubmission_list.info()\nsubmission_list.to_csv('/kaggle/working/predictions_for_submission', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T03:28:11.481376Z","iopub.execute_input":"2023-10-01T03:28:11.481813Z","iopub.status.idle":"2023-10-01T03:28:11.504092Z","shell.execute_reply.started":"2023-10-01T03:28:11.481776Z","shell.execute_reply":"2023-10-01T03:28:11.502397Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype\n---  ------  --------------  -----\n 0   id      3263 non-null   int64\n 1   target  3263 non-null   int64\ndtypes: int64(2)\nmemory usage: 51.1 KB\n","output_type":"stream"}]}]}
